{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Class Classification with `Trainer` and `OIG-moderation` dataset\n",
    "## Based on Huggingface Sagemaker-sdk - Getting Started Demo\n",
    "#### Ed Lee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Introduction](#Introduction)  \n",
    "2. [Development Environment and Permissions](#Development-Environment-and-Permissions)\n",
    "    1. [Installation](#Installation)  \n",
    "    2. [Development environment](#Development-environment)  \n",
    "    3. [Permissions](#Permissions)\n",
    "3. [Processing](#Preprocessing)   \n",
    "    1. [Tokenization](#Tokenization)  \n",
    "    2. [Uploading data to sagemaker_session_bucket](#Uploading-data-to-sagemaker_session_bucket)  \n",
    "4. [Fine-tuning & starting Sagemaker Training Job](#Fine-tuning-\\&-starting-Sagemaker-Training-Job)  \n",
    "    1. [Creating an Estimator and start a training job](#Creating-an-Estimator-and-start-a-training-job)  \n",
    "    2. [Estimator Parameters](#Estimator-Parameters)   \n",
    "    3. [Download fine-tuned model from s3](#Download-fine-tuned-model-from-s3)\n",
    "    3. [Attach to old training job to an estimator ](#Attach-to-old-training-job-to-an-estimator)  \n",
    "5. [_Coming soon_:Push model to the Hugging Face hub](#Push-model-to-the-Hugging-Face-hub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Welcome to our end-to-end Multi-Class Text-Classification example. In this demo, we will use the Hugging Faces `transformers` and `datasets` library together with a custom Amazon sagemaker-sdk extension to fine-tune a pre-trained transformer on muti-class text classification. In particular, the pre-trained model will be fine-tuned using the `OIG-moderation` dataset. To get started, we need to set up the environment with a few prerequisite steps, for permissions, configurations, and so on. \n",
    "\n",
    "_**NOTE: You can run this demo in Sagemaker Studio, your local machine or Sagemaker Notebook Instances**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development Environment and Permissions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Installation\n",
    "\n",
    "* Select notebook kernel on Sagemaker: PyTorch 1.13 Python 3.9\n",
    "\n",
    "_*Note:* we only install the required libraries from Hugging Face and AWS. You also need PyTorch or Tensorflow, if you haven´t it installed_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker>=2.140.0 in /opt/conda/lib/python3.9/site-packages (2.194.0)\n",
      "Requirement already satisfied: transformers==4.26.1 in /opt/conda/lib/python3.9/site-packages (4.26.1)\n",
      "Requirement already satisfied: datasets[s3]==2.10.1 in /opt/conda/lib/python3.9/site-packages (2.10.1)\n",
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.1-py3-none-any.whl (139 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.4/139.4 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.26.1) (0.17.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.26.1) (0.13.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.26.1) (1.23.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.26.1) (6.0.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers==4.26.1) (2.28.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.26.1) (2023.10.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.26.1) (23.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers==4.26.1) (3.12.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers==4.26.1) (4.64.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from datasets[s3]==2.10.1) (0.70.14)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.9/site-packages (from datasets[s3]==2.10.1) (0.3.6)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from datasets[s3]==2.10.1) (1.5.3)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.9/site-packages (from datasets[s3]==2.10.1) (2023.1.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from datasets[s3]==2.10.1) (3.8.6)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.9/site-packages (from datasets[s3]==2.10.1) (0.18.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets[s3]==2.10.1) (11.0.0)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from datasets[s3]==2.10.1) (3.4.1)\n",
      "Requirement already satisfied: s3fs in /opt/conda/lib/python3.9/site-packages (from datasets[s3]==2.10.1) (0.4.2)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.9/site-packages (from sagemaker>=2.140.0) (3.11.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.26.131 in /opt/conda/lib/python3.9/site-packages (from sagemaker>=2.140.0) (1.28.70)\n",
      "Requirement already satisfied: tblib==1.7.0 in /opt/conda/lib/python3.9/site-packages (from sagemaker>=2.140.0) (1.7.0)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.9/site-packages (from sagemaker>=2.140.0) (1.0.1)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.9/site-packages (from sagemaker>=2.140.0) (4.19.1)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.9/site-packages (from sagemaker>=2.140.0) (0.2.0)\n",
      "Requirement already satisfied: attrs<24,>=23.1.0 in /opt/conda/lib/python3.9/site-packages (from sagemaker>=2.140.0) (23.1.0)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /opt/conda/lib/python3.9/site-packages (from sagemaker>=2.140.0) (4.13.0)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /opt/conda/lib/python3.9/site-packages (from sagemaker>=2.140.0) (2.2.1)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.9/site-packages (from sagemaker>=2.140.0) (0.3.0)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /opt/conda/lib/python3.9/site-packages (from sagemaker>=2.140.0) (3.20.2)\n",
      "Requirement already satisfied: schema in /opt/conda/lib/python3.9/site-packages (from sagemaker>=2.140.0) (0.7.5)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.9/site-packages (from ipywidgets) (5.9.0)\n",
      "Collecting jupyterlab-widgets~=3.0.9\n",
      "  Downloading jupyterlab_widgets-3.0.9-py3-none-any.whl (214 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.9/214.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting widgetsnbextension~=4.0.9\n",
      "  Downloading widgetsnbextension-4.0.9-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting comm>=0.1.3\n",
      "  Downloading comm-0.1.4-py3-none-any.whl (6.6 kB)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.9/site-packages (from ipywidgets) (8.10.0)\n",
      "Requirement already satisfied: s3transfer<0.8.0,>=0.7.0 in /opt/conda/lib/python3.9/site-packages (from boto3<2.0,>=1.26.131->sagemaker>=2.140.0) (0.7.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.9/site-packages (from boto3<2.0,>=1.26.131->sagemaker>=2.140.0) (1.0.1)\n",
      "Requirement already satisfied: botocore<1.32.0,>=1.31.70 in /opt/conda/lib/python3.9/site-packages (from boto3<2.0,>=1.26.131->sagemaker>=2.140.0) (1.31.70)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets[s3]==2.10.1) (2.1.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets[s3]==2.10.1) (1.9.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets[s3]==2.10.1) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets[s3]==2.10.1) (4.0.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets[s3]==2.10.1) (6.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets[s3]==2.10.1) (1.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.26.1) (4.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.9/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker>=2.140.0) (3.13.0)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.30 in /opt/conda/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.36)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.2)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (2.14.0)\n",
      "Requirement already satisfied: stack-data in /opt/conda/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.26.1) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.26.1) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.26.1) (3.4)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.9/site-packages (from google-pasta->sagemaker>=2.140.0) (1.16.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.9/site-packages (from jsonschema->sagemaker>=2.140.0) (0.10.6)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.9/site-packages (from jsonschema->sagemaker>=2.140.0) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.9/site-packages (from jsonschema->sagemaker>=2.140.0) (0.30.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets[s3]==2.10.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets[s3]==2.10.1) (2022.7.1)\n",
      "Requirement already satisfied: pox>=0.3.2 in /opt/conda/lib/python3.9/site-packages (from pathos->sagemaker>=2.140.0) (0.3.2)\n",
      "Requirement already satisfied: ppft>=1.7.6.6 in /opt/conda/lib/python3.9/site-packages (from pathos->sagemaker>=2.140.0) (1.7.6.6)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.9/site-packages (from schema->sagemaker>=2.140.0) (21.6.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.9/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.9/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.9/site-packages (from prompt-toolkit<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets) (0.2.8)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: pure-eval in /opt/conda/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Installing collected packages: widgetsnbextension, jupyterlab-widgets, comm, ipywidgets\n",
      "Successfully installed comm-0.1.4 ipywidgets-8.1.1 jupyterlab-widgets-3.0.9 widgetsnbextension-4.0.9\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install \"sagemaker>=2.140.0\" \"transformers==4.26.1\" \"datasets[s3]==2.10.1\" \"ipywidgets\" --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker.huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_If you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker role arn: arn:aws:iam::391015568214:role/service-role/AmazonSageMaker-ExecutionRole-20231024T120029\n",
      "sagemaker bucket: sagemaker-us-east-2-391015568214\n",
      "sagemaker session region: us-east-2\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "We are using the `datasets` library to download and preprocess the `OIG-moderation` dataset. After preprocessing, the dataset will be uploaded to our `sagemaker_session_bucket` to be used within our training job. \n",
    "\n",
    "https://huggingface.co/datasets/ontocord/OIG-moderation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset \n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# tokenizer used in preprocessing\n",
    "tokenizer_name = 'distilbert-base-uncased'\n",
    "\n",
    "# dataset used\n",
    "dataset_name = 'ontocord/OIG-moderation'\n",
    "\n",
    "# s3 key prefix for the data\n",
    "s3_prefix = 'samples/datasets/OIG-moderation'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Must Login to Hugging Face Hub  to Download OIG-moderation\n",
    "Please create an API key in Settings -> Access Tokens and enter when prompted below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7ecba9fe3394748861635b77a3ae289",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/root/.cache/huggingface/datasets/ontocord___json/ontocord--OIG-moderation-0dac4560d8f6e324/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30d67cc7970244319fd33fef6d39bbeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# If the dataset is gated/private, make sure you have run huggingface-cli login\n",
    "\n",
    "loaded_dataset = load_dataset(dataset_name) # smaller the size for dataset to 10k for faster iterating\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Cleaning for the OIG-moderation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sample down for faster iteration, ok for illustrative purposes.\n",
    "SAMPLE_SIZE = 20000\n",
    "CLASS_LABEL_COL = 'str_labels'\n",
    "orig_df = pd.DataFrame(loaded_dataset).sample(n=SAMPLE_SIZE, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29d268b65fd346b8ba88f530111555cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/16060 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "543e17ad58274fef8dee5ac6615b2898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16060 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_parsed_df(orig_df):\n",
    "    parsed_df = orig_df.copy()\n",
    "    parsed_df['old_text'] = parsed_df['train'].apply(lambda x: x.get(\"text\"))\n",
    "    split_text = parsed_df['old_text'].str.split('\\n')\n",
    "    parsed_df['text'] = split_text.str[0].str.split(':').str[1].str.strip()\n",
    "    parsed_df[CLASS_LABEL_COL] = split_text.str[1].str.split(':').str[1].str.strip()\n",
    "    return parsed_df\n",
    "\n",
    "def get_cleaned_labels_df(orig_df):\n",
    "    \"\"\"Remove examples that are of very low count.\"\"\"\n",
    "    MIN_IN_CATEGORY = 1000\n",
    "    df_low_value_counts = orig_df.groupby(CLASS_LABEL_COL).count()\n",
    "    filter_df = df_low_value_counts.reset_index()[df_low_value_counts.reset_index().train > MIN_IN_CATEGORY]\n",
    "    return orig_df.merge(filter_df[CLASS_LABEL_COL], on=CLASS_LABEL_COL, how='inner')\n",
    "\n",
    "def get_remove_missing_and_fix_spelling_df(orig_df):\n",
    "    df = orig_df.copy()\n",
    "    df = df[['text', CLASS_LABEL_COL]]\n",
    "    df = orig_df[orig_df[CLASS_LABEL_COL].str.len() > 0]\n",
    "    df[CLASS_LABEL_COL] = df[CLASS_LABEL_COL].replace({\"causual\": \"casual\"})  # replace spelling mistake in outcomes\n",
    "    df = df[df['text'].notna()]  # exclude rows that are NA\n",
    "    df = df.loc[df['text'].str.strip() != '', :]  # exclude rows with empty string\n",
    "    return df\n",
    "\n",
    "df = orig_df.pipe(get_parsed_df).pipe(get_cleaned_labels_df).pipe(get_remove_missing_and_fix_spelling_df)\n",
    "df = df[['text', CLASS_LABEL_COL]]\n",
    "\n",
    "df.groupby(CLASS_LABEL_COL).count().plot.bar()\n",
    "\n",
    "dataset = Dataset.from_pandas(df, preserve_index=False)\n",
    "dataset = dataset.class_encode_column(CLASS_LABEL_COL)\n",
    "\n",
    "# Need to represent labels columns as tensors\n",
    "labels = [label for label in dataset.features[CLASS_LABEL_COL].names]\n",
    "id2label = {idx:label for idx, label in enumerate(labels)}\n",
    "label2id = {label:idx for idx, label in enumerate(labels)}\n",
    "\n",
    "def to_onehot_tensor(row):\n",
    "    \"\"\" 1-hot encodes a tensor \"\"\"\n",
    "    # Training won't run if don't cast as floats \n",
    "    # https://discuss.pytorch.org/t/multi-label-binary-classification-result-type-float-cant-be-cast-to-the-desired-output-type-long/117915\n",
    "    row['labels_onehot'] = np.eye(len(labels), dtype='uint8')[row[CLASS_LABEL_COL]].astype(np.float32).tolist()\n",
    "    return row\n",
    "\n",
    "dataset = dataset.map(to_onehot_tensor, batched=True)\n",
    "dataset = dataset.rename_column(\"labels_onehot\", \"labels\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b60d8e69c0c5446a8a1a8a5227ff482b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12848 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1aff1b370fa4431a0c9745ce9ccc1dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3212 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# download tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding='max_length', truncation=True)\n",
    "\n",
    "split_ds = dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "# tokenize dataset\n",
    "train_dataset = split_ds['train'].map(tokenize, batched=True)\n",
    "test_dataset = split_ds['test'].map(tokenize, batched=True)\n",
    "\n",
    "# # set format for pytorch\n",
    "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-26 19:08:43.212: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using NamedTuple = typing._NamedTuple instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-26 19:08:43.240 pytorch-1-13-gpu-py-ml-g4dn-xlarge-93fa10c7f16e176d479999e56114:2106 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2023-10-26 19:08:43.259 pytorch-1-13-gpu-py-ml-g4dn-xlarge-93fa10c7f16e176d479999e56114:2106 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'labels': tensor([[0., 1., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 1., 0., 0.],\n",
       "         [0., 0., 0., 0., 1., 0.],\n",
       "         [0., 1., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0., 0.]]),\n",
       " 'input_ids': tensor([[  101,  3057,  2360,  ...,     0,     0,     0],\n",
       "         [  101,  2062,  2013,  ...,     0,     0,     0],\n",
       "         [  101,  2054,  2079,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  2016, 10855,  ...,     0,     0,     0],\n",
       "         [  101,  1045,  2074,  ...,     0,     0,     0],\n",
       "         [  101,  2204,  2559,  ...,     0,     0,     0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the data\n",
    "dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=10)\n",
    "next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " more details about your research'}] more details about your research'}]## Uploading data to `sagemaker_session_bucket`\n",
    "\n",
    "After we processed the `datasets` we are going to use the new `FileSystem` [integration](https://huggingface.co/docs/datasets/filesystems.html) to upload our dataset to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3ea2b6a7be04cd0896abb61b4d599d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/12848 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a44b05a7730412898c6f748b158ba3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/3212 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# save train_dataset to s3\n",
    "training_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/train'\n",
    "train_dataset.save_to_disk(training_input_path)\n",
    "\n",
    "# save test_dataset to s3\n",
    "test_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/test'\n",
    "test_dataset.save_to_disk(test_input_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning & starting Sagemaker Training Job\n",
    "\n",
    "In order to create a sagemaker training job we need an `HuggingFace` Estimator. The Estimator handles end-to-end Amazon SageMaker training and deployment tasks. In a Estimator we define, which fine-tuning script should be used as `entry_point`, which `instance_type` should be used, which `hyperparameters` are passed in .....\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "huggingface_estimator = HuggingFace(entry_point='train_multiclass.py',\n",
    "                            source_dir='./scripts',\n",
    "                            base_job_name='huggingface-sdk-extension',\n",
    "                            instance_type='ml.p3.2xlarge',\n",
    "                            instance_count=1,\n",
    "                            transformers_version='4.4',\n",
    "                            pytorch_version='1.6',\n",
    "                            py_version='py36',\n",
    "                            role=role,\n",
    "                            hyperparameters = {'epochs': 1,\n",
    "                                               'train_batch_size': 32,\n",
    "                                               'model_name':'distilbert-base-uncased'\n",
    "                                                })\n",
    "```\n",
    "\n",
    "When we create a SageMaker training job, SageMaker takes care of starting and managing all the required ec2 instances for us with the `huggingface` container, uploads the provided fine-tuning script `train.py` and downloads the data from our `sagemaker_session_bucket` into the container at `/opt/ml/input/data`. Then, it starts the training job by running. \n",
    "\n",
    "```python\n",
    "/opt/conda/bin/python train.py --epochs 1 --model_name distilbert-base-uncased --train_batch_size 32\n",
    "```\n",
    "\n",
    "The `hyperparameters` you define in the `HuggingFace` estimator are passed in as named arguments. \n",
    "\n",
    "Sagemaker is providing useful properties about the training environment through various environment variables, including the following:\n",
    "\n",
    "* `SM_MODEL_DIR`: A string that represents the path where the training job writes the model artifacts to. After training, artifacts in this directory are uploaded to S3 for model hosting.\n",
    "\n",
    "* `SM_NUM_GPUS`: An integer representing the number of GPUs available to the host.\n",
    "\n",
    "* `SM_CHANNEL_XXXX:` A string that represents the path to the directory that contains the input data for the specified channel. For example, if you specify two input channels in the HuggingFace estimator’s fit call, named `train` and `test`, the environment variables `SM_CHANNEL_TRAIN` and `SM_CHANNEL_TEST` are set.\n",
    "\n",
    "\n",
    "To run your training job locally you can define `instance_type='local'` or `instance_type='local_gpu'` for gpu usage. _Note: this does not working within SageMaker Studio_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m AutoModelForSequenceClassification, Trainer, TrainingArguments, AutoTokenizer\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m EvalPrediction\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmetrics\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m accuracy_score, precision_recall_fscore_support\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmetrics\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m f1_score, roc_auc_score, accuracy_score\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdatasets\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m load_from_disk\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mrandom\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    parser = argparse.ArgumentParser()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# hyperparameters sent by the client are passed as command-line arguments to the script.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m3\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--train_batch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m32\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--eval_batch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m64\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--warmup_steps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m500\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--model_name\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--learning_rate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[34m5e-5\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Data, model, and output directories\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--output_data_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_OUTPUT_DATA_DIR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--model_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--n_gpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_NUM_GPUS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--training_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAIN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--test_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_TEST\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    args, _ = parser.parse_known_args()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Set up logging\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    logger = logging.getLogger(\u001b[31m__name__\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    logging.basicConfig(\u001b[37m\u001b[39;49;00m\n",
      "        level=logging.getLevelName(\u001b[33m\"\u001b[39;49;00m\u001b[33mINFO\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m),\u001b[37m\u001b[39;49;00m\n",
      "        handlers=[logging.StreamHandler(sys.stdout)],\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mformat\u001b[39;49;00m=\u001b[33m\"\u001b[39;49;00m\u001b[33m%(asctime)s\u001b[39;49;00m\u001b[33m - \u001b[39;49;00m\u001b[33m%(name)s\u001b[39;49;00m\u001b[33m - \u001b[39;49;00m\u001b[33m%(levelname)s\u001b[39;49;00m\u001b[33m - \u001b[39;49;00m\u001b[33m%(message)s\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# load datasets\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    train_dataset = load_from_disk(args.training_dir)\u001b[37m\u001b[39;49;00m\n",
      "    test_dataset = load_from_disk(args.test_dir)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m loaded train_dataset length is: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[36mlen\u001b[39;49;00m(train_dataset)\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m loaded test_dataset length is: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[36mlen\u001b[39;49;00m(test_dataset)\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# compute metrics function for binary classification\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# def compute_metrics(pred):\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m#     labels = pred.label_ids\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m#     preds = pred.predictions.argmax(-1)\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m#     precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\")\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m#     acc = accuracy_score(labels, preds)\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m#     return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mmulti_label_metrics\u001b[39;49;00m(predictions, labels, threshold=\u001b[34m0.5\u001b[39;49;00m):\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m# first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        sigmoid = torch.nn.Sigmoid()\u001b[37m\u001b[39;49;00m\n",
      "        probs = sigmoid(torch.Tensor(predictions))\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m# next, use threshold to turn them into integer predictions\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        y_pred = np.zeros(probs.shape)\u001b[37m\u001b[39;49;00m\n",
      "        y_pred[np.where(probs >= threshold)] = \u001b[34m1\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m# finally, compute metrics\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        y_true = labels\u001b[37m\u001b[39;49;00m\n",
      "        f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average=\u001b[33m'\u001b[39;49;00m\u001b[33mmicro\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        roc_auc = roc_auc_score(y_true, y_pred, average = \u001b[33m'\u001b[39;49;00m\u001b[33mmicro\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        accuracy = accuracy_score(y_true, y_pred)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m# return as dictionary\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        metrics = {\u001b[33m'\u001b[39;49;00m\u001b[33mf1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: f1_micro_average,\u001b[37m\u001b[39;49;00m\n",
      "                   \u001b[33m'\u001b[39;49;00m\u001b[33mroc_auc\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: roc_auc,\u001b[37m\u001b[39;49;00m\n",
      "                   \u001b[33m'\u001b[39;49;00m\u001b[33maccuracy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: accuracy}\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mreturn\u001b[39;49;00m metrics\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mcompute_metrics\u001b[39;49;00m(p: EvalPrediction):\u001b[37m\u001b[39;49;00m\n",
      "        preds = p.predictions[\u001b[34m0\u001b[39;49;00m] \u001b[34mif\u001b[39;49;00m \u001b[36misinstance\u001b[39;49;00m(p.predictions, \u001b[37m\u001b[39;49;00m\n",
      "                \u001b[36mtuple\u001b[39;49;00m) \u001b[34melse\u001b[39;49;00m p.predictions\u001b[37m\u001b[39;49;00m\n",
      "        result = multi_label_metrics(\u001b[37m\u001b[39;49;00m\n",
      "            predictions=preds, \u001b[37m\u001b[39;49;00m\n",
      "            labels=p.label_ids)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mreturn\u001b[39;49;00m result\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Set up multi-class model\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    labels = [label \u001b[34mfor\u001b[39;49;00m label \u001b[35min\u001b[39;49;00m train_dataset.features[\u001b[33m'\u001b[39;49;00m\u001b[33mstr_labels\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].names]\u001b[37m\u001b[39;49;00m\n",
      "    id2label = {idx:label \u001b[34mfor\u001b[39;49;00m idx, label \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(labels)}\u001b[37m\u001b[39;49;00m\n",
      "    label2id = {label:idx \u001b[34mfor\u001b[39;49;00m idx, label \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(labels)}\u001b[37m\u001b[39;49;00m\n",
      "    model = AutoModelForSequenceClassification.from_pretrained(\u001b[37m\u001b[39;49;00m\n",
      "        args.model_name, \u001b[37m\u001b[39;49;00m\n",
      "        problem_type=\u001b[33m\"\u001b[39;49;00m\u001b[33mmulti_label_classification\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[37m\u001b[39;49;00m\n",
      "        num_labels=\u001b[36mlen\u001b[39;49;00m(labels),\u001b[37m\u001b[39;49;00m\n",
      "        id2label=id2label,\u001b[37m\u001b[39;49;00m\n",
      "        label2id=label2id)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n",
      "    tokenizer = AutoTokenizer.from_pretrained(args.model_name)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# define training args\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    training_args = TrainingArguments(\u001b[37m\u001b[39;49;00m\n",
      "        output_dir=args.model_dir,\u001b[37m\u001b[39;49;00m\n",
      "        num_train_epochs=args.epochs,\u001b[37m\u001b[39;49;00m\n",
      "        per_device_train_batch_size=args.train_batch_size,\u001b[37m\u001b[39;49;00m\n",
      "        per_device_eval_batch_size=args.eval_batch_size,\u001b[37m\u001b[39;49;00m\n",
      "        warmup_steps=args.warmup_steps,\u001b[37m\u001b[39;49;00m\n",
      "        evaluation_strategy=\u001b[33m\"\u001b[39;49;00m\u001b[33mepoch\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        logging_strategy=\u001b[33m\"\u001b[39;49;00m\u001b[33mepoch\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        logging_dir=\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.output_data_dir\u001b[33m}\u001b[39;49;00m\u001b[33m/logs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        learning_rate=\u001b[36mfloat\u001b[39;49;00m(args.learning_rate),\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# create Trainer instance\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    trainer = Trainer(\u001b[37m\u001b[39;49;00m\n",
      "        model=model,\u001b[37m\u001b[39;49;00m\n",
      "        args=training_args,\u001b[37m\u001b[39;49;00m\n",
      "        compute_metrics=compute_metrics,\u001b[37m\u001b[39;49;00m\n",
      "        train_dataset=train_dataset,\u001b[37m\u001b[39;49;00m\n",
      "        eval_dataset=test_dataset,\u001b[37m\u001b[39;49;00m\n",
      "        tokenizer=tokenizer,\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# train model\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    trainer.train()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# evaluate model\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    eval_result = trainer.evaluate(eval_dataset=test_dataset)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# writes eval result to file which can be accessed later in s3 ouput\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(os.path.join(args.output_data_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33meval_results.txt\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m), \u001b[33m\"\u001b[39;49;00m\u001b[33mw\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m writer:\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m***** Eval results *****\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mfor\u001b[39;49;00m key, value \u001b[35min\u001b[39;49;00m \u001b[36msorted\u001b[39;49;00m(eval_result.items()):\u001b[37m\u001b[39;49;00m\n",
      "            writer.write(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mkey\u001b[33m}\u001b[39;49;00m\u001b[33m = \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mvalue\u001b[33m}\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Saves the model to s3\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    trainer.save_model(args.model_dir)\u001b[37m\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "!pygmentize ./scripts/train_multiclass.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an Estimator and start a training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={'epochs': 3,\n",
    "                 'train_batch_size': 32,\n",
    "                 'model_name':'distilbert-base-uncased'\n",
    "                 }\n",
    "\n",
    "instance_type = 'ml.p3.16xlarge'\n",
    "# instance_type = 'ml.p3.2xlarge'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "huggingface_estimator = HuggingFace(entry_point='train_multiclass.py',\n",
    "                            source_dir='./scripts',\n",
    "                            instance_type=instance_type,\n",
    "                            instance_count=1,\n",
    "                            role=role,\n",
    "                            transformers_version='4.26',\n",
    "                            pytorch_version='1.13',\n",
    "                            py_version='py39',\n",
    "                            hyperparameters = hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ff0000; text-decoration-color: #ff0000\">╭──────────────────────────────────────────────────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\"> &lt;ipython-input-70-b9a693e3f86f&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">39</span>                                                              <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">▲</span>                                                                                                <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">SyntaxError: </span>keyword argument repeated: instance_type\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[91m╭──────────────────────────────────────────────────────────────────────────────────────────────────╮\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[33m <ipython-input-70-b9a693e3f86f>\u001b[0m:\u001b[94m39\u001b[0m                                                              \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                                  \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[1;91m▲\u001b[0m                                                                                                \u001b[91m│\u001b[0m\n",
       "\u001b[91m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mSyntaxError: \u001b[0mkeyword argument repeated: instance_type\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# configuration for running training on smdistributed Data Parallel\n",
    "\n",
    "distribution = {'smdistributed':{'dataparallel':{ 'enabled': True }}}\n",
    "instance_type = 'ml.p3.16xlarge'\n",
    "instance_count = 2\n",
    "\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={\n",
    "    'model_id': tokenizer_name,\n",
    "    'epochs': 3,\n",
    "    'per_device_train_batch_size': 32,\n",
    "    'per_device_eval_batch_size': 32,\n",
    "    'learning_rate': 3e-5,\n",
    "    'fp16': True,\n",
    "    'save_total_limit':2,\n",
    "    'load_best_model_at_end':True,\n",
    "    'metric_for_best_model':\"f1\",\n",
    "    'doc_stride': 128,\n",
    "    'pad_to_max_length': True,\n",
    "    'output_dir': '/opt/ml/model'\n",
    "}\n",
    "\n",
    "# configuration for running training on smdistributed Data Parallel\n",
    "distribution = {'smdistributed':{'dataparallel':{ 'enabled': True }}}\n",
    "\n",
    "\n",
    "git_config = {'repo': 'https://github.com/huggingface/transformers.git','branch': 'v4.26.0'}\n",
    "metric_definitions=[\n",
    "     {\"Name\": \"train_runtime\", \"Regex\": \"train_runtime.*=\\D*(.*?)$\"},\n",
    "     {'Name': 'train_samples_per_second', 'Regex': \"train_samples_per_second.*=\\D*(.*?)$\"},\n",
    "     {'Name': 'epoch', 'Regex': \"epoch.*=\\D*(.*?)$\"},\n",
    "     {'Name': 'f1', 'Regex': \"f1.*=\\D*(.*?)$\"},\n",
    "     {'Name': 'exact_match', 'Regex': \"exact_match.*=\\D*(.*?)$\"}]\n",
    "volume_size=200\n",
    "\n",
    "huggingface_estimator = HuggingFace(\n",
    "                            entry_point='train_multiclass.py',\n",
    "                            source_dir='./scripts',\n",
    "                            instance_type='ml.p3.16xlarge',\n",
    "                            git_config=git_config,\n",
    "                            metric_definitions=metric_definitions,\n",
    "                            instance_type=instance_type,\n",
    "                            instance_count=instance_count,\n",
    "                            volume_size=volume_size,\n",
    "                            role=role,\n",
    "                            transformers_version='4.26.0',\n",
    "                            pytorch_version='1.13.1',\n",
    "                            py_version='py39',\n",
    "                            distribution=distribution,\n",
    "                            hyperparameters=hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an Estimator and Start Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-pytorch-training-2023-10-26-19-12-25-308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n",
      "2023-10-26 19:12:25 Starting - Starting the training job......\n",
      "2023-10-26 19:13:22 Starting - Preparing the instances for training.........\n",
      "2023-10-26 19:14:35 Downloading - Downloading input data...\n",
      "2023-10-26 19:15:00 Training - Downloading the training image.................................\n",
      "2023-10-26 19:20:47 Training - Training image download completed. Training in progress...\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-10-26 19:21:11,409 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-10-26 19:21:11,471 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-10-26 19:21:11,483 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-10-26 19:21:11,486 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-10-26 19:21:11,818 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-10-26 19:21:11,894 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-10-26 19:21:11,969 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-10-26 19:21:11,982 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p3.16xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 3,\n",
      "        \"model_name\": \"distilbert-base-uncased\",\n",
      "        \"train_batch_size\": 32\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p3.16xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2023-10-26-19-12-25-308\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-2-391015568214/huggingface-pytorch-training-2023-10-26-19-12-25-308/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_multiclass\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 64,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.16xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.16xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train_multiclass.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":3,\"model_name\":\"distilbert-base-uncased\",\"train_batch_size\":32}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train_multiclass.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.16xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.16xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p3.16xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.16xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train_multiclass\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=64\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-2-391015568214/huggingface-pytorch-training-2023-10-26-19-12-25-308/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.p3.16xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":3,\"model_name\":\"distilbert-base-uncased\",\"train_batch_size\":32},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.16xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"huggingface-pytorch-training-2023-10-26-19-12-25-308\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-2-391015568214/huggingface-pytorch-training-2023-10-26-19-12-25-308/source/sourcedir.tar.gz\",\"module_name\":\"train_multiclass\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.16xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.16xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_multiclass.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"3\",\"--model_name\",\"distilbert-base-uncased\",\"--train_batch_size\",\"32\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=3\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME=distilbert-base-uncased\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_BATCH_SIZE=32\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.9 train_multiclass.py --epochs 3 --model_name distilbert-base-uncased --train_batch_size 32\u001b[0m\n",
      "\u001b[34m[2023-10-26 19:21:14.149: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m2023-10-26 19:21:14,156 root         INFO     Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m2023-10-26 19:21:14,187 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34m2023-10-26 19:21:17,487 - __main__ - INFO -  loaded train_dataset length is: 12848\u001b[0m\n",
      "\u001b[34m2023-10-26 19:21:17,487 - __main__ - INFO -  loaded test_dataset length is: 3212\u001b[0m\n",
      "\u001b[34mDownloading (…)lve/main/config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)lve/main/config.json: 100%|██████████| 483/483 [00:00<00:00, 63.3kB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/268M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"pytorch_model.bin\";:  16%|█▌        | 41.9M/268M [00:00<00:00, 376MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"pytorch_model.bin\";:  35%|███▌      | 94.4M/268M [00:00<00:00, 425MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"pytorch_model.bin\";:  55%|█████▍    | 147M/268M [00:00<00:00, 444MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"pytorch_model.bin\";:  74%|███████▍  | 199M/268M [00:00<00:00, 452MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"pytorch_model.bin\";:  94%|█████████▍| 252M/268M [00:00<00:00, 460MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"pytorch_model.bin\";: 100%|██████████| 268M/268M [00:00<00:00, 445MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias', 'classifier.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias', 'classifier.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mDownloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)okenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 9.46kB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 41.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)/main/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 15.2MB/s]\u001b[0m\n",
      "\u001b[34mThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: str_labels, text. If str_labels, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34mThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: str_labels, text. If str_labels, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m***** Running training *****\u001b[0m\n",
      "\u001b[34m***** Running training *****\u001b[0m\n",
      "\u001b[34mNum examples = 12848\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\u001b[0m\n",
      "\u001b[34mNum examples = 12848\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\u001b[0m\n",
      "\u001b[34mTotal train batch size (w. parallel, distributed & accumulation) = 256\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 153\u001b[0m\n",
      "\u001b[34mTotal train batch size (w. parallel, distributed & accumulation) = 256\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 153\u001b[0m\n",
      "\u001b[34mNumber of trainable parameters = 66958086\u001b[0m\n",
      "\u001b[34mNumber of trainable parameters = 66958086\u001b[0m\n",
      "\u001b[34m0%|          | 0/153 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[2023-10-26 19:21:22.079: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m2023-10-26 19:21:22,086 - root - INFO - Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[2023-10-26 19:21:22.121 algo-1:105 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-10-26 19:21:22.160 algo-1:105 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-10-26 19:21:22.161 algo-1:105 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-10-26 19:21:22.161 algo-1:105 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-10-26 19:21:22.161 algo-1:105 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-10-26 19:21:22.161 algo-1:105 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34mYou're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\u001b[0m\n",
      "\u001b[34mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[34malgo-1:105:299 [0] ofi_init:1304 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34malgo-1:105:299 [0] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m1%|          | 1/153 [00:15<39:09, 15.46s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 2/153 [00:16<17:03,  6.78s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 3/153 [00:16<09:59,  4.00s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 4/153 [00:17<06:41,  2.69s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 5/153 [00:18<04:51,  1.97s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 6/153 [00:18<03:45,  1.54s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 7/153 [00:19<03:03,  1.26s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 8/153 [00:20<02:36,  1.08s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 9/153 [00:21<02:17,  1.04it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 10/153 [00:21<02:05,  1.14it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 11/153 [00:22<01:59,  1.19it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 12/153 [00:23<01:52,  1.26it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 13/153 [00:23<01:47,  1.31it/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 14/153 [00:24<01:43,  1.35it/s]\u001b[0m\n",
      "\u001b[34m10%|▉         | 15/153 [00:25<01:40,  1.37it/s]\u001b[0m\n",
      "\u001b[34m10%|█         | 16/153 [00:25<01:38,  1.39it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 17/153 [00:26<01:36,  1.41it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 18/153 [00:27<01:35,  1.42it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 19/153 [00:28<01:34,  1.42it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 20/153 [00:28<01:33,  1.43it/s]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 21/153 [00:29<01:32,  1.43it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 22/153 [00:30<01:31,  1.44it/s]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 23/153 [00:30<01:30,  1.44it/s]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 24/153 [00:31<01:29,  1.44it/s]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 25/153 [00:32<01:28,  1.44it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 26/153 [00:32<01:28,  1.44it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 27/153 [00:33<01:27,  1.44it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 28/153 [00:34<01:26,  1.44it/s]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 29/153 [00:34<01:25,  1.44it/s]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 30/153 [00:35<01:25,  1.44it/s]\u001b[0m\n",
      "\u001b[34m20%|██        | 31/153 [00:36<01:24,  1.44it/s]\u001b[0m\n",
      "\u001b[34m21%|██        | 32/153 [00:37<01:23,  1.44it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 33/153 [00:37<01:26,  1.39it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 34/153 [00:38<01:24,  1.41it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 35/153 [00:39<01:23,  1.42it/s]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 36/153 [00:39<01:22,  1.42it/s]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 37/153 [00:40<01:21,  1.43it/s]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 38/153 [00:41<01:20,  1.43it/s]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 39/153 [00:41<01:19,  1.43it/s]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 40/153 [00:42<01:18,  1.43it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 41/153 [00:43<01:18,  1.43it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 42/153 [00:44<01:17,  1.44it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 43/153 [00:44<01:16,  1.44it/s]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 44/153 [00:45<01:15,  1.44it/s]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 45/153 [00:46<01:14,  1.44it/s]\u001b[0m\n",
      "\u001b[34m30%|███       | 46/153 [00:46<01:14,  1.44it/s]\u001b[0m\n",
      "\u001b[34m31%|███       | 47/153 [00:47<01:13,  1.44it/s]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 48/153 [00:48<01:12,  1.44it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 49/153 [00:48<01:12,  1.44it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 50/153 [00:49<01:11,  1.44it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 51/153 [00:49<00:57,  1.78it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6542, 'learning_rate': 5.1e-06, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 51/153 [00:49<00:57,  1.78it/s]\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: str_labels, text. If str_labels, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: str_labels, text. If str_labels, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 3212\n",
      "  Batch size = 512\u001b[0m\n",
      "\u001b[34mNum examples = 3212\n",
      "  Batch size = 512\u001b[0m\n",
      "\u001b[34m0%|          | 0/7 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m29%|██▊       | 2/7 [00:00<00:01,  3.13it/s]#033[A\u001b[0m\n",
      "\u001b[34m43%|████▎     | 3/7 [00:01<00:01,  2.22it/s]#033[A\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 4/7 [00:01<00:01,  1.92it/s]#033[A\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 5/7 [00:02<00:01,  1.79it/s]#033[A\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 6/7 [00:03<00:00,  1.71it/s]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 7/7 [00:03<00:00,  2.08it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.5791289806365967, 'eval_f1': 0.04585218702865762, 'eval_roc_auc': 0.5109900373599005, 'eval_accuracy': 0.023661270236612703, 'eval_runtime': 4.0993, 'eval_samples_per_second': 783.543, 'eval_steps_per_second': 1.708, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 51/153 [00:53<00:57,  1.78it/s]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 7/7 [00:03<00:00,  2.08it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\u001b[0m\n",
      "\u001b[34m34%|███▍      | 52/153 [00:54<03:07,  1.86s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 53/153 [00:55<02:30,  1.51s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 54/153 [00:56<02:05,  1.26s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 55/153 [00:56<01:47,  1.09s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 56/153 [00:57<01:34,  1.03it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 57/153 [00:58<01:25,  1.12it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 58/153 [00:58<01:18,  1.20it/s]\u001b[0m\n",
      "\u001b[34m39%|███▊      | 59/153 [00:59<01:14,  1.27it/s]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 60/153 [01:00<01:10,  1.31it/s]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 61/153 [01:00<01:08,  1.35it/s]\u001b[0m\n",
      "\u001b[34m41%|████      | 62/153 [01:01<01:06,  1.38it/s]\u001b[0m\n",
      "\u001b[34m41%|████      | 63/153 [01:02<01:04,  1.39it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 64/153 [01:03<01:03,  1.41it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 65/153 [01:03<01:02,  1.42it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 66/153 [01:04<01:01,  1.42it/s]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 67/153 [01:05<01:00,  1.43it/s]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 68/153 [01:05<00:59,  1.43it/s]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 69/153 [01:06<00:58,  1.43it/s]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 70/153 [01:07<00:57,  1.43it/s]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 71/153 [01:07<00:57,  1.44it/s]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 72/153 [01:08<00:56,  1.44it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 73/153 [01:09<00:55,  1.44it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 74/153 [01:10<00:54,  1.45it/s]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 75/153 [01:10<00:53,  1.45it/s]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 76/153 [01:11<00:53,  1.45it/s]\u001b[0m\n",
      "\u001b[34m50%|█████     | 77/153 [01:12<00:52,  1.45it/s]\u001b[0m\n",
      "\u001b[34m51%|█████     | 78/153 [01:12<00:51,  1.45it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 79/153 [01:13<00:51,  1.45it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 80/153 [01:14<00:51,  1.43it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 81/153 [01:14<00:50,  1.43it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 82/153 [01:15<00:49,  1.44it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 83/153 [01:16<00:48,  1.44it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 84/153 [01:16<00:47,  1.44it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 85/153 [01:17<00:47,  1.44it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 86/153 [01:18<00:46,  1.44it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 87/153 [01:19<00:45,  1.44it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 88/153 [01:19<00:45,  1.44it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 89/153 [01:20<00:44,  1.44it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 90/153 [01:21<00:43,  1.44it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 91/153 [01:21<00:42,  1.44it/s]\u001b[0m\n",
      "\u001b[34m60%|██████    | 92/153 [01:22<00:42,  1.44it/s]\u001b[0m\n",
      "\u001b[34m61%|██████    | 93/153 [01:23<00:41,  1.45it/s]\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 94/153 [01:23<00:40,  1.45it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 95/153 [01:24<00:40,  1.45it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 96/153 [01:25<00:39,  1.45it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 97/153 [01:25<00:38,  1.45it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 98/153 [01:26<00:37,  1.45it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 99/153 [01:27<00:37,  1.45it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 100/153 [01:27<00:36,  1.45it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 101/153 [01:28<00:35,  1.45it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 102/153 [01:28<00:28,  1.79it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4776, 'learning_rate': 1.02e-05, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 102/153 [01:28<00:28,  1.79it/s]\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: str_labels, text. If str_labels, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: str_labels, text. If str_labels, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 3212\n",
      "  Batch size = 512\u001b[0m\n",
      "\u001b[34mNum examples = 3212\n",
      "  Batch size = 512\u001b[0m\n",
      "\u001b[34m0%|          | 0/7 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m29%|██▊       | 2/7 [00:00<00:01,  2.72it/s]#033[A\u001b[0m\n",
      "\u001b[34m43%|████▎     | 3/7 [00:01<00:01,  2.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 4/7 [00:02<00:01,  1.86it/s]#033[A\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 5/7 [00:02<00:01,  1.75it/s]#033[A\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 6/7 [00:03<00:00,  1.68it/s]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 7/7 [00:03<00:00,  2.04it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.4041157066822052, 'eval_f1': 0.028737389177621524, 'eval_roc_auc': 0.5069427148194271, 'eval_accuracy': 0.014632627646326276, 'eval_runtime': 4.1946, 'eval_samples_per_second': 765.739, 'eval_steps_per_second': 1.669, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 102/153 [01:33<00:28,  1.79it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 7/7 [00:03<00:00,  2.04it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 103/153 [01:33<01:32,  1.86s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 104/153 [01:34<01:13,  1.51s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 105/153 [01:35<01:00,  1.26s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 106/153 [01:35<00:51,  1.09s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 107/153 [01:36<00:44,  1.03it/s]\u001b[0m\n",
      "\u001b[34m71%|███████   | 108/153 [01:37<00:39,  1.13it/s]\u001b[0m\n",
      "\u001b[34m71%|███████   | 109/153 [01:37<00:36,  1.21it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 110/153 [01:38<00:33,  1.27it/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 111/153 [01:39<00:31,  1.32it/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 112/153 [01:40<00:30,  1.35it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 113/153 [01:40<00:29,  1.38it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 114/153 [01:41<00:27,  1.40it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 115/153 [01:42<00:26,  1.41it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 116/153 [01:42<00:26,  1.42it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 117/153 [01:43<00:25,  1.42it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 118/153 [01:44<00:24,  1.43it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 119/153 [01:44<00:23,  1.43it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 120/153 [01:45<00:22,  1.44it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 121/153 [01:46<00:22,  1.44it/s]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 122/153 [01:46<00:21,  1.45it/s]\u001b[0m\n",
      "\u001b[34m80%|████████  | 123/153 [01:47<00:20,  1.45it/s]\u001b[0m\n",
      "\u001b[34m81%|████████  | 124/153 [01:48<00:20,  1.45it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 125/153 [01:49<00:19,  1.45it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 126/153 [01:49<00:18,  1.45it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 127/153 [01:50<00:17,  1.45it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▎ | 128/153 [01:51<00:17,  1.45it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 129/153 [01:51<00:16,  1.45it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 130/153 [01:52<00:15,  1.45it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 131/153 [01:53<00:15,  1.45it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▋ | 132/153 [01:53<00:14,  1.45it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 133/153 [01:54<00:13,  1.45it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 134/153 [01:55<00:13,  1.45it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 135/153 [01:55<00:12,  1.45it/s]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 136/153 [01:56<00:11,  1.45it/s]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 137/153 [01:57<00:11,  1.45it/s]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 138/153 [01:58<00:10,  1.45it/s]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 139/153 [01:58<00:09,  1.45it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 140/153 [01:59<00:08,  1.45it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 141/153 [02:00<00:08,  1.45it/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 142/153 [02:00<00:07,  1.45it/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 143/153 [02:01<00:06,  1.45it/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 144/153 [02:02<00:06,  1.45it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 145/153 [02:02<00:05,  1.40it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 146/153 [02:03<00:04,  1.41it/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 147/153 [02:04<00:04,  1.42it/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 148/153 [02:04<00:03,  1.43it/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 149/153 [02:05<00:02,  1.44it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 150/153 [02:06<00:02,  1.44it/s]\u001b[0m\n",
      "\u001b[34m99%|█████████▊| 151/153 [02:07<00:01,  1.44it/s]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 152/153 [02:07<00:00,  1.44it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 153/153 [02:08<00:00,  1.78it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 0.3772, 'learning_rate': 1.53e-05, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 153/153 [02:08<00:00,  1.78it/s]\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: str_labels, text. If str_labels, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: str_labels, text. If str_labels, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 3212\n",
      "  Batch size = 512\u001b[0m\n",
      "\u001b[34mNum examples = 3212\n",
      "  Batch size = 512\u001b[0m\n",
      "\u001b[34m0%|          | 0/7 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m29%|██▊       | 2/7 [00:00<00:01,  3.17it/s]#033[A\u001b[0m\n",
      "\u001b[34m43%|████▎     | 3/7 [00:01<00:01,  2.25it/s]#033[A\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 4/7 [00:01<00:01,  1.95it/s]#033[A\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 5/7 [00:02<00:01,  1.81it/s]#033[A\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 6/7 [00:03<00:00,  1.73it/s]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 7/7 [00:03<00:00,  2.10it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.33002036809921265, 'eval_f1': 0.5470368835474514, 'eval_roc_auc': 0.6963262764632627, 'eval_accuracy': 0.4066002490660025, 'eval_runtime': 4.0606, 'eval_samples_per_second': 791.02, 'eval_steps_per_second': 1.724, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 153/153 [02:12<00:00,  1.78it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 7/7 [00:03<00:00,  2.10it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m{'train_runtime': 132.0668, 'train_samples_per_second': 291.852, 'train_steps_per_second': 1.159, 'train_loss': 0.503003263785169, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 153/153 [02:12<00:00,  1.78it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 153/153 [02:12<00:00,  1.16it/s]\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: str_labels, text. If str_labels, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: str_labels, text. If str_labels, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 3212\n",
      "  Batch size = 512\u001b[0m\n",
      "\u001b[34mNum examples = 3212\n",
      "  Batch size = 512\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\u001b[0m\n",
      "\u001b[34m0%|          | 0/7 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 2/7 [00:00<00:01,  3.20it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 3/7 [00:01<00:01,  2.26it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 4/7 [00:01<00:01,  1.95it/s]\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 5/7 [00:02<00:01,  1.81it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 6/7 [00:03<00:00,  1.73it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 7/7 [00:03<00:00,  2.10it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 7/7 [00:03<00:00,  2.05it/s]\u001b[0m\n",
      "\u001b[34m***** Eval results *****\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m2023-10-26 19:23:40,762 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-10-26 19:23:40,762 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-10-26 19:23:40,763 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-10-26 19:23:59 Uploading - Uploading generated training model\n",
      "2023-10-26 19:23:59 Completed - Training job completed\n",
      "Training seconds: 564\n",
      "Billable seconds: 564\n"
     ]
    }
   ],
   "source": [
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit({'train': training_input_path, 'test': test_input_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the endpoint\n",
    "\n",
    "To deploy our endpoint, we call `deploy()` on our HuggingFace estimator object, passing in our desired number of instances and instance type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: huggingface-pytorch-training-2023-10-26-19-25-59-385\n",
      "INFO:sagemaker:Creating endpoint-config with name huggingface-pytorch-training-2023-10-26-19-25-59-385\n",
      "INFO:sagemaker:Creating endpoint with name huggingface-pytorch-training-2023-10-26-19-25-59-385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "predictor = huggingface_estimator.deploy(1, \"ml.g4dn.xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we use the returned predictor object to call the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'casual', 'score': 0.7036333084106445}]\n",
      "[{'label': 'needs caution', 'score': 0.6842705607414246}]\n"
     ]
    }
   ],
   "source": [
    "happy_input = {\"inputs\": \"Good morning how are you today?\"}\n",
    "black_eye_pea_lyrics = {\"inputs\": \"Overseas, yeah, we tryna stop terrorism but we still got terrorists here livin' in the USA, the big CIA The Bloods and the Crips and the KKK\"}\n",
    "\n",
    "print(predictor.predict(happy_input))\n",
    "print(predictor.predict(black_eye_pea_lyrics))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we delete the endpoint again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting model with name: huggingface-pytorch-training-2023-10-25-06-52-57-698\n",
      "INFO:sagemaker:Deleting endpoint configuration with name: huggingface-pytorch-training-2023-10-25-06-52-57-698\n",
      "INFO:sagemaker:Deleting endpoint with name: huggingface-pytorch-training-2023-10-25-06-52-57-698\n"
     ]
    }
   ],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimator Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "container image used for training job: \n",
      "None\n",
      "\n",
      "s3 uri where the trained model is located: \n",
      "s3://sagemaker-us-east-2-391015568214/huggingface-pytorch-training-2023-10-25-06-39-03-508/output/model.tar.gz\n",
      "\n",
      "latest training job name for this estimator: \n",
      "huggingface-pytorch-training-2023-10-25-06-39-03-508\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# container image used for training job\n",
    "print(f\"container image used for training job: \\n{huggingface_estimator.image_uri}\\n\")\n",
    "\n",
    "# s3 uri where the trained model is located\n",
    "print(f\"s3 uri where the trained model is located: \\n{huggingface_estimator.model_data}\\n\")\n",
    "\n",
    "# latest training job name for this estimator\n",
    "print(f\"latest training job name for this estimator: \\n{huggingface_estimator.latest_training_job.name}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-25 06:52:03 Starting - Preparing the instances for training\n",
      "2023-10-25 06:52:03 Downloading - Downloading input data\n",
      "2023-10-25 06:52:03 Training - Training image download completed. Training in progress.\n",
      "2023-10-25 06:52:03 Uploading - Uploading generated training model\n",
      "2023-10-25 06:52:03 Completed - Training job completed\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-10-25 06:47:43,564 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-10-25 06:47:43,585 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-10-25 06:47:43,598 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-10-25 06:47:43,601 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-10-25 06:47:43,891 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-10-25 06:47:43,928 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-10-25 06:47:43,967 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-10-25 06:47:43,982 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 1,\n",
      "        \"model_name\": \"distilbert-base-uncased\",\n",
      "        \"train_batch_size\": 32\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p3.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2023-10-25-06-39-03-508\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-2-391015568214/huggingface-pytorch-training-2023-10-25-06-39-03-508/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_multiclass\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train_multiclass.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":1,\"model_name\":\"distilbert-base-uncased\",\"train_batch_size\":32}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train_multiclass.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p3.2xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train_multiclass\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-2-391015568214/huggingface-pytorch-training-2023-10-25-06-39-03-508/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.p3.2xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":1,\"model_name\":\"distilbert-base-uncased\",\"train_batch_size\":32},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"huggingface-pytorch-training-2023-10-25-06-39-03-508\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-2-391015568214/huggingface-pytorch-training-2023-10-25-06-39-03-508/source/sourcedir.tar.gz\",\"module_name\":\"train_multiclass\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_multiclass.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"1\",\"--model_name\",\"distilbert-base-uncased\",\"--train_batch_size\",\"32\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME=distilbert-base-uncased\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_BATCH_SIZE=32\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.9 train_multiclass.py --epochs 1 --model_name distilbert-base-uncased --train_batch_size 32\u001b[0m\n",
      "\u001b[34m[2023-10-25 06:47:46.172: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m2023-10-25 06:47:46,179 root         INFO     Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m2023-10-25 06:47:46,211 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34m2023-10-25 06:47:49,671 - __main__ - INFO -  loaded train_dataset length is: 12797\u001b[0m\n",
      "\u001b[34m2023-10-25 06:47:49,671 - __main__ - INFO -  loaded test_dataset length is: 3200\u001b[0m\n",
      "\u001b[34mDownloading (…)lve/main/config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)lve/main/config.json: 100%|██████████| 483/483 [00:00<00:00, 60.3kB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/268M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"pytorch_model.bin\";:  16%|█▌        | 41.9M/268M [00:00<00:00, 338MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"pytorch_model.bin\";:  35%|███▌      | 94.4M/268M [00:00<00:00, 396MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"pytorch_model.bin\";:  55%|█████▍    | 147M/268M [00:00<00:00, 413MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"pytorch_model.bin\";:  70%|███████   | 189M/268M [00:00<00:00, 415MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"pytorch_model.bin\";:  90%|█████████ | 241M/268M [00:00<00:00, 418MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"pytorch_model.bin\";: 100%|██████████| 268M/268M [00:00<00:00, 407MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mDownloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)okenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 9.64kB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 10.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)/main/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 14.4MB/s]\u001b[0m\n",
      "\u001b[34mThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: str_labels, text. If str_labels, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34mThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: str_labels, text. If str_labels, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m***** Running training *****\u001b[0m\n",
      "\u001b[34mNum examples = 12797\u001b[0m\n",
      "\u001b[34m***** Running training *****\n",
      "  Num examples = 12797\u001b[0m\n",
      "\u001b[34mNum Epochs = 1\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\u001b[0m\n",
      "\u001b[34mGradient Accumulation steps = 1\n",
      "  Total optimization steps = 400\u001b[0m\n",
      "\u001b[34mNum Epochs = 1\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 400\u001b[0m\n",
      "\u001b[34mNumber of trainable parameters = 66958086\u001b[0m\n",
      "\u001b[34mNumber of trainable parameters = 66958086\u001b[0m\n",
      "\u001b[34m0%|          | 0/400 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[2023-10-25 06:47:53.912: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m2023-10-25 06:47:53,919 - root - INFO - Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[2023-10-25 06:47:53.955 algo-1:49 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-10-25 06:47:53.994 algo-1:49 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-10-25 06:47:53.995 algo-1:49 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-10-25 06:47:53.995 algo-1:49 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-10-25 06:47:53.996 algo-1:49 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-10-25 06:47:53.996 algo-1:49 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34mYou're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m0%|          | 1/400 [00:01<11:09,  1.68s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 2/400 [00:02<06:21,  1.04it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 3/400 [00:02<04:49,  1.37it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 4/400 [00:03<04:05,  1.61it/s]\u001b[0m\n",
      "\u001b[34m1%|▏         | 5/400 [00:03<03:41,  1.78it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 6/400 [00:03<03:26,  1.90it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 7/400 [00:04<03:17,  1.99it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 8/400 [00:04<03:11,  2.04it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 9/400 [00:05<03:07,  2.09it/s]\u001b[0m\n",
      "\u001b[34m2%|▎         | 10/400 [00:05<03:04,  2.12it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 11/400 [00:06<03:02,  2.14it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 12/400 [00:06<03:00,  2.15it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 13/400 [00:07<02:58,  2.17it/s]\u001b[0m\n",
      "\u001b[34m4%|▎         | 14/400 [00:07<02:57,  2.17it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 15/400 [00:08<02:57,  2.17it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 16/400 [00:08<02:56,  2.18it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 17/400 [00:08<02:56,  2.17it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 18/400 [00:09<02:55,  2.17it/s]\u001b[0m\n",
      "\u001b[34m5%|▍         | 19/400 [00:09<02:55,  2.17it/s]\u001b[0m\n",
      "\u001b[34m5%|▌         | 20/400 [00:10<02:54,  2.18it/s]\u001b[0m\n",
      "\u001b[34m5%|▌         | 21/400 [00:10<02:53,  2.19it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 22/400 [00:11<02:52,  2.19it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 23/400 [00:11<02:52,  2.19it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 24/400 [00:12<02:51,  2.19it/s]\u001b[0m\n",
      "\u001b[34m6%|▋         | 25/400 [00:12<02:51,  2.19it/s]\u001b[0m\n",
      "\u001b[34m6%|▋         | 26/400 [00:13<02:50,  2.19it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 27/400 [00:13<02:50,  2.19it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 28/400 [00:14<02:50,  2.19it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 29/400 [00:14<02:49,  2.19it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 30/400 [00:14<02:48,  2.19it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 31/400 [00:15<02:48,  2.19it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 32/400 [00:15<02:48,  2.19it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 33/400 [00:16<02:47,  2.19it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 34/400 [00:16<02:47,  2.19it/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 35/400 [00:17<02:46,  2.19it/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 36/400 [00:17<02:46,  2.19it/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 37/400 [00:18<02:45,  2.19it/s]\u001b[0m\n",
      "\u001b[34m10%|▉         | 38/400 [00:18<02:45,  2.18it/s]\u001b[0m\n",
      "\u001b[34m10%|▉         | 39/400 [00:19<02:45,  2.18it/s]\u001b[0m\n",
      "\u001b[34m10%|█         | 40/400 [00:19<02:44,  2.18it/s]\u001b[0m\n",
      "\u001b[34m10%|█         | 41/400 [00:19<02:44,  2.19it/s]\u001b[0m\n",
      "\u001b[34m10%|█         | 42/400 [00:20<02:43,  2.19it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 43/400 [00:20<02:42,  2.19it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 44/400 [00:21<02:42,  2.19it/s]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 45/400 [00:21<02:42,  2.19it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 46/400 [00:22<02:41,  2.19it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 47/400 [00:22<02:41,  2.19it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 48/400 [00:23<02:40,  2.19it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 49/400 [00:23<02:40,  2.19it/s]\u001b[0m\n",
      "\u001b[34m12%|█▎        | 50/400 [00:24<02:40,  2.18it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 51/400 [00:24<02:40,  2.18it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 52/400 [00:24<02:39,  2.18it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 53/400 [00:25<02:38,  2.19it/s]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 54/400 [00:25<02:38,  2.19it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 55/400 [00:26<02:37,  2.19it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 56/400 [00:26<02:37,  2.18it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 57/400 [00:27<02:36,  2.19it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 58/400 [00:27<02:36,  2.18it/s]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 59/400 [00:28<02:36,  2.18it/s]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 60/400 [00:28<02:35,  2.18it/s]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 61/400 [00:29<02:35,  2.18it/s]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 62/400 [00:29<02:35,  2.18it/s]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 63/400 [00:30<02:34,  2.18it/s]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 64/400 [00:30<02:33,  2.19it/s]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 65/400 [00:30<02:33,  2.19it/s]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 66/400 [00:31<02:33,  2.18it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 67/400 [00:31<02:32,  2.18it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 68/400 [00:32<02:32,  2.17it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 69/400 [00:32<02:31,  2.18it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 70/400 [00:33<02:31,  2.18it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 71/400 [00:33<02:30,  2.18it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 72/400 [00:34<02:30,  2.18it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 73/400 [00:34<02:29,  2.19it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 74/400 [00:35<02:29,  2.19it/s]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 75/400 [00:35<02:28,  2.18it/s]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 76/400 [00:35<02:28,  2.18it/s]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 77/400 [00:36<02:27,  2.18it/s]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 78/400 [00:36<02:27,  2.18it/s]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 79/400 [00:37<02:27,  2.18it/s]\u001b[0m\n",
      "\u001b[34m20%|██        | 80/400 [00:37<02:26,  2.18it/s]\u001b[0m\n",
      "\u001b[34m20%|██        | 81/400 [00:38<02:26,  2.18it/s]\u001b[0m\n",
      "\u001b[34m20%|██        | 82/400 [00:38<02:25,  2.19it/s]\u001b[0m\n",
      "\u001b[34m21%|██        | 83/400 [00:39<02:24,  2.19it/s]\u001b[0m\n",
      "\u001b[34m21%|██        | 84/400 [00:39<02:24,  2.19it/s]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 85/400 [00:40<02:23,  2.19it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 86/400 [00:40<02:23,  2.19it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 87/400 [00:41<02:23,  2.19it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 88/400 [00:41<02:23,  2.18it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 89/400 [00:41<02:23,  2.17it/s]\u001b[0m\n",
      "\u001b[34m22%|██▎       | 90/400 [00:42<02:22,  2.18it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 91/400 [00:42<02:21,  2.18it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 92/400 [00:43<02:21,  2.18it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 93/400 [00:43<02:20,  2.18it/s]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 94/400 [00:44<02:20,  2.18it/s]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 95/400 [00:44<02:19,  2.18it/s]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 96/400 [00:45<02:19,  2.18it/s]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 97/400 [00:45<02:19,  2.18it/s]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 98/400 [00:46<02:19,  2.17it/s]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 99/400 [00:46<02:18,  2.17it/s]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 100/400 [00:47<02:18,  2.17it/s]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 101/400 [00:47<02:17,  2.17it/s]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 102/400 [00:47<02:17,  2.17it/s]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 103/400 [00:48<02:16,  2.18it/s]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 104/400 [00:48<02:16,  2.17it/s]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 105/400 [00:49<02:15,  2.18it/s]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 106/400 [00:49<02:14,  2.18it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 107/400 [00:50<02:14,  2.18it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 108/400 [00:50<02:13,  2.18it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 109/400 [00:51<02:13,  2.19it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 110/400 [00:51<02:12,  2.19it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 111/400 [00:52<02:12,  2.18it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 112/400 [00:52<02:12,  2.18it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 113/400 [00:52<02:11,  2.18it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 114/400 [00:53<02:11,  2.18it/s]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 115/400 [00:53<02:10,  2.18it/s]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 116/400 [00:54<02:10,  2.18it/s]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 117/400 [00:54<02:09,  2.18it/s]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 118/400 [00:55<02:09,  2.18it/s]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 119/400 [00:55<02:08,  2.18it/s]\u001b[0m\n",
      "\u001b[34m30%|███       | 120/400 [00:56<02:08,  2.18it/s]\u001b[0m\n",
      "\u001b[34m30%|███       | 121/400 [00:56<02:07,  2.18it/s]\u001b[0m\n",
      "\u001b[34m30%|███       | 122/400 [00:57<02:07,  2.18it/s]\u001b[0m\n",
      "\u001b[34m31%|███       | 123/400 [00:57<02:06,  2.18it/s]\u001b[0m\n",
      "\u001b[34m31%|███       | 124/400 [00:58<02:06,  2.18it/s]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 125/400 [00:58<02:05,  2.19it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 126/400 [00:58<02:05,  2.19it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 127/400 [00:59<02:05,  2.18it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 128/400 [00:59<02:04,  2.18it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 129/400 [01:00<02:04,  2.18it/s]\u001b[0m\n",
      "\u001b[34m32%|███▎      | 130/400 [01:00<02:03,  2.18it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 131/400 [01:01<02:03,  2.18it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 132/400 [01:01<02:02,  2.19it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 133/400 [01:02<02:02,  2.19it/s]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 134/400 [01:02<02:01,  2.19it/s]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 135/400 [01:03<02:01,  2.18it/s]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 136/400 [01:03<02:01,  2.18it/s]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 137/400 [01:03<02:01,  2.17it/s]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 138/400 [01:04<02:00,  2.17it/s]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 139/400 [01:04<01:59,  2.18it/s]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 140/400 [01:05<01:59,  2.18it/s]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 141/400 [01:05<01:58,  2.18it/s]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 142/400 [01:06<01:58,  2.18it/s]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 143/400 [01:06<01:57,  2.18it/s]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 144/400 [01:07<01:57,  2.19it/s]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 145/400 [01:07<01:56,  2.19it/s]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 146/400 [01:08<01:56,  2.18it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 147/400 [01:08<01:55,  2.18it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 148/400 [01:09<01:55,  2.18it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 149/400 [01:09<01:55,  2.18it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 150/400 [01:09<01:54,  2.18it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 151/400 [01:10<01:54,  2.18it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 152/400 [01:10<01:53,  2.18it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 153/400 [01:11<01:52,  2.19it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 154/400 [01:11<01:52,  2.18it/s]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 155/400 [01:12<01:52,  2.18it/s]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 156/400 [01:12<01:52,  2.18it/s]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 157/400 [01:13<01:51,  2.17it/s]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 158/400 [01:13<01:51,  2.18it/s]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 159/400 [01:14<01:50,  2.17it/s]\u001b[0m\n",
      "\u001b[34m40%|████      | 160/400 [01:14<01:50,  2.17it/s]\u001b[0m\n",
      "\u001b[34m40%|████      | 161/400 [01:14<01:49,  2.17it/s]\u001b[0m\n",
      "\u001b[34m40%|████      | 162/400 [01:15<01:49,  2.18it/s]\u001b[0m\n",
      "\u001b[34m41%|████      | 163/400 [01:15<01:49,  2.17it/s]\u001b[0m\n",
      "\u001b[34m41%|████      | 164/400 [01:16<01:48,  2.18it/s]\u001b[0m\n",
      "\u001b[34m41%|████▏     | 165/400 [01:16<01:47,  2.18it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 166/400 [01:17<01:47,  2.17it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 167/400 [01:17<01:47,  2.18it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 168/400 [01:18<01:46,  2.18it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 169/400 [01:18<01:45,  2.18it/s]\u001b[0m\n",
      "\u001b[34m42%|████▎     | 170/400 [01:19<01:45,  2.18it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 171/400 [01:19<01:45,  2.18it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 172/400 [01:20<01:44,  2.18it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 173/400 [01:20<01:44,  2.18it/s]\u001b[0m\n",
      "\u001b[34m44%|████▎     | 174/400 [01:20<01:43,  2.18it/s]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 175/400 [01:21<01:43,  2.17it/s]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 176/400 [01:21<01:43,  2.17it/s]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 177/400 [01:22<01:42,  2.18it/s]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 178/400 [01:22<01:41,  2.18it/s]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 179/400 [01:23<01:41,  2.18it/s]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 180/400 [01:23<01:40,  2.19it/s]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 181/400 [01:24<01:40,  2.19it/s]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 182/400 [01:24<01:40,  2.18it/s]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 183/400 [01:25<01:39,  2.18it/s]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 184/400 [01:25<01:38,  2.19it/s]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 185/400 [01:25<01:38,  2.19it/s]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 186/400 [01:26<01:37,  2.19it/s]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 187/400 [01:26<01:37,  2.19it/s]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 188/400 [01:27<01:36,  2.19it/s]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 189/400 [01:27<01:36,  2.19it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 190/400 [01:28<01:36,  2.18it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 191/400 [01:28<01:35,  2.18it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 192/400 [01:29<01:35,  2.18it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 193/400 [01:29<01:34,  2.19it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 194/400 [01:30<01:34,  2.19it/s]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 195/400 [01:30<01:33,  2.18it/s]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 196/400 [01:31<01:33,  2.18it/s]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 197/400 [01:31<01:33,  2.18it/s]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 198/400 [01:31<01:32,  2.18it/s]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 199/400 [01:32<01:32,  2.18it/s]\u001b[0m\n",
      "\u001b[34m50%|█████     | 200/400 [01:32<01:31,  2.18it/s]\u001b[0m\n",
      "\u001b[34m50%|█████     | 201/400 [01:33<01:31,  2.18it/s]\u001b[0m\n",
      "\u001b[34m50%|█████     | 202/400 [01:33<01:30,  2.18it/s]\u001b[0m\n",
      "\u001b[34m51%|█████     | 203/400 [01:34<01:30,  2.19it/s]\u001b[0m\n",
      "\u001b[34m51%|█████     | 204/400 [01:34<01:29,  2.18it/s]\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 205/400 [01:35<01:29,  2.18it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 206/400 [01:35<01:29,  2.18it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 207/400 [01:36<01:28,  2.18it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 208/400 [01:36<01:27,  2.18it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 209/400 [01:36<01:27,  2.19it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▎    | 210/400 [01:37<01:26,  2.19it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 211/400 [01:37<01:26,  2.19it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 212/400 [01:38<01:25,  2.19it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 213/400 [01:38<01:25,  2.19it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 214/400 [01:39<01:25,  2.18it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 215/400 [01:39<01:24,  2.18it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 216/400 [01:40<01:24,  2.18it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 217/400 [01:40<01:23,  2.19it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 218/400 [01:41<01:23,  2.19it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 219/400 [01:41<01:22,  2.18it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 220/400 [01:42<01:22,  2.18it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 221/400 [01:42<01:21,  2.18it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 222/400 [01:42<01:21,  2.18it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 223/400 [01:43<01:21,  2.18it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 224/400 [01:43<01:21,  2.17it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 225/400 [01:44<01:20,  2.17it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 226/400 [01:44<01:20,  2.17it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 227/400 [01:45<01:19,  2.17it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 228/400 [01:45<01:19,  2.18it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 229/400 [01:46<01:18,  2.18it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▊    | 230/400 [01:46<01:17,  2.18it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 231/400 [01:47<01:17,  2.18it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 232/400 [01:47<01:17,  2.18it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 233/400 [01:47<01:16,  2.18it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 234/400 [01:48<01:16,  2.17it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 235/400 [01:48<01:16,  2.17it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 236/400 [01:49<01:15,  2.17it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 237/400 [01:49<01:15,  2.17it/s]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 238/400 [01:50<01:14,  2.18it/s]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 239/400 [01:50<01:13,  2.18it/s]\u001b[0m\n",
      "\u001b[34m60%|██████    | 240/400 [01:51<01:13,  2.17it/s]\u001b[0m\n",
      "\u001b[34m60%|██████    | 241/400 [01:51<01:13,  2.17it/s]\u001b[0m\n",
      "\u001b[34m60%|██████    | 242/400 [01:52<01:12,  2.17it/s]\u001b[0m\n",
      "\u001b[34m61%|██████    | 243/400 [01:52<01:12,  2.17it/s]\u001b[0m\n",
      "\u001b[34m61%|██████    | 244/400 [01:53<01:11,  2.17it/s]\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 245/400 [01:53<01:11,  2.17it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 246/400 [01:53<01:10,  2.18it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 247/400 [01:54<01:10,  2.18it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 248/400 [01:54<01:09,  2.18it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 249/400 [01:55<01:09,  2.18it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▎   | 250/400 [01:55<01:08,  2.18it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 251/400 [01:56<01:08,  2.18it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 252/400 [01:56<01:07,  2.18it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 253/400 [01:57<01:07,  2.18it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▎   | 254/400 [01:57<01:06,  2.18it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 255/400 [01:58<01:06,  2.18it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 256/400 [01:58<01:06,  2.18it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 257/400 [01:59<01:05,  2.18it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 258/400 [01:59<01:05,  2.18it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 259/400 [01:59<01:04,  2.19it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 260/400 [02:00<01:04,  2.19it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 261/400 [02:00<01:03,  2.18it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 262/400 [02:01<01:03,  2.19it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 263/400 [02:01<01:02,  2.19it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 264/400 [02:02<01:02,  2.19it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 265/400 [02:02<01:01,  2.19it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 266/400 [02:03<01:01,  2.18it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 267/400 [02:03<01:00,  2.18it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 268/400 [02:04<01:00,  2.18it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 269/400 [02:04<01:00,  2.17it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 270/400 [02:04<00:59,  2.18it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 271/400 [02:05<00:59,  2.18it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 272/400 [02:05<00:58,  2.18it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 273/400 [02:06<00:58,  2.18it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 274/400 [02:06<00:57,  2.18it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 275/400 [02:07<00:57,  2.17it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 276/400 [02:07<00:57,  2.17it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 277/400 [02:08<00:56,  2.16it/s]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 278/400 [02:08<00:56,  2.17it/s]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 279/400 [02:09<00:55,  2.17it/s]\u001b[0m\n",
      "\u001b[34m70%|███████   | 280/400 [02:09<00:55,  2.17it/s]\u001b[0m\n",
      "\u001b[34m70%|███████   | 281/400 [02:10<00:55,  2.16it/s]\u001b[0m\n",
      "\u001b[34m70%|███████   | 282/400 [02:10<00:54,  2.17it/s]\u001b[0m\n",
      "\u001b[34m71%|███████   | 283/400 [02:10<00:54,  2.17it/s]\u001b[0m\n",
      "\u001b[34m71%|███████   | 284/400 [02:11<00:53,  2.17it/s]\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 285/400 [02:11<00:52,  2.17it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 286/400 [02:12<00:52,  2.17it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 287/400 [02:12<00:52,  2.17it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 288/400 [02:13<00:51,  2.17it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 289/400 [02:13<00:51,  2.17it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▎  | 290/400 [02:14<00:50,  2.18it/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 291/400 [02:14<00:50,  2.17it/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 292/400 [02:15<00:49,  2.17it/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 293/400 [02:15<00:49,  2.17it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 294/400 [02:16<00:48,  2.18it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 295/400 [02:16<00:48,  2.18it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 296/400 [02:16<00:47,  2.18it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 297/400 [02:17<00:47,  2.18it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 298/400 [02:17<00:46,  2.18it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 299/400 [02:18<00:46,  2.18it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 300/400 [02:18<00:45,  2.18it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 301/400 [02:19<00:45,  2.18it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 302/400 [02:19<00:44,  2.18it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 303/400 [02:20<00:44,  2.18it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 304/400 [02:20<00:44,  2.18it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 305/400 [02:21<00:43,  2.18it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 306/400 [02:21<00:43,  2.18it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 307/400 [02:21<00:42,  2.18it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 308/400 [02:22<00:42,  2.18it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 309/400 [02:22<00:41,  2.18it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 310/400 [02:23<00:41,  2.18it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 311/400 [02:23<00:40,  2.18it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 312/400 [02:24<00:40,  2.18it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 313/400 [02:24<00:40,  2.17it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 314/400 [02:25<00:39,  2.17it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 315/400 [02:25<00:39,  2.17it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 316/400 [02:26<00:38,  2.17it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 317/400 [02:26<00:38,  2.17it/s]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 318/400 [02:27<00:37,  2.18it/s]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 319/400 [02:27<00:37,  2.18it/s]\u001b[0m\n",
      "\u001b[34m80%|████████  | 320/400 [02:27<00:36,  2.18it/s]\u001b[0m\n",
      "\u001b[34m80%|████████  | 321/400 [02:28<00:36,  2.17it/s]\u001b[0m\n",
      "\u001b[34m80%|████████  | 322/400 [02:28<00:36,  2.17it/s]\u001b[0m\n",
      "\u001b[34m81%|████████  | 323/400 [02:29<00:35,  2.17it/s]\u001b[0m\n",
      "\u001b[34m81%|████████  | 324/400 [02:29<00:35,  2.17it/s]\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 325/400 [02:30<00:34,  2.17it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 326/400 [02:30<00:34,  2.17it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 327/400 [02:31<00:33,  2.17it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 328/400 [02:31<00:33,  2.17it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 329/400 [02:32<00:32,  2.17it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▎ | 330/400 [02:32<00:32,  2.17it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 331/400 [02:33<00:31,  2.17it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 332/400 [02:33<00:31,  2.17it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 333/400 [02:33<00:30,  2.17it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▎ | 334/400 [02:34<00:30,  2.17it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 335/400 [02:34<00:29,  2.17it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 336/400 [02:35<00:29,  2.18it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 337/400 [02:35<00:28,  2.18it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 338/400 [02:36<00:28,  2.18it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 339/400 [02:36<00:27,  2.18it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 340/400 [02:37<00:27,  2.18it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 341/400 [02:37<00:27,  2.18it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 342/400 [02:38<00:26,  2.18it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 343/400 [02:38<00:26,  2.18it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 344/400 [02:39<00:25,  2.18it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▋ | 345/400 [02:39<00:25,  2.18it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▋ | 346/400 [02:39<00:24,  2.17it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 347/400 [02:40<00:24,  2.17it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 348/400 [02:40<00:23,  2.17it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 349/400 [02:41<00:23,  2.17it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 350/400 [02:41<00:23,  2.17it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 351/400 [02:42<00:22,  2.17it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 352/400 [02:42<00:22,  2.17it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 353/400 [02:43<00:21,  2.17it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 354/400 [02:43<00:21,  2.17it/s]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 355/400 [02:44<00:20,  2.17it/s]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 356/400 [02:44<00:20,  2.17it/s]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 357/400 [02:44<00:19,  2.17it/s]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 358/400 [02:45<00:19,  2.17it/s]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 359/400 [02:45<00:18,  2.17it/s]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 360/400 [02:46<00:18,  2.18it/s]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 361/400 [02:46<00:17,  2.18it/s]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 362/400 [02:47<00:17,  2.18it/s]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 363/400 [02:47<00:17,  2.17it/s]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 364/400 [02:48<00:16,  2.18it/s]\u001b[0m\n",
      "\u001b[34m91%|█████████▏| 365/400 [02:48<00:16,  2.17it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 366/400 [02:49<00:15,  2.17it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 367/400 [02:49<00:15,  2.17it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 368/400 [02:50<00:14,  2.17it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 369/400 [02:50<00:14,  2.17it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▎| 370/400 [02:50<00:13,  2.17it/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 371/400 [02:51<00:13,  2.18it/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 372/400 [02:51<00:12,  2.18it/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 373/400 [02:52<00:12,  2.18it/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▎| 374/400 [02:52<00:11,  2.18it/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 375/400 [02:53<00:11,  2.18it/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 376/400 [02:53<00:10,  2.18it/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 377/400 [02:54<00:10,  2.18it/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 378/400 [02:54<00:10,  2.18it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 379/400 [02:55<00:09,  2.18it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 380/400 [02:55<00:09,  2.18it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 381/400 [02:56<00:08,  2.18it/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 382/400 [02:56<00:08,  2.17it/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 383/400 [02:56<00:07,  2.17it/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 384/400 [02:57<00:07,  2.17it/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▋| 385/400 [02:57<00:06,  2.17it/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▋| 386/400 [02:58<00:06,  2.18it/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 387/400 [02:58<00:05,  2.18it/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 388/400 [02:59<00:05,  2.18it/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 389/400 [02:59<00:05,  2.18it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 390/400 [03:00<00:04,  2.18it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 391/400 [03:00<00:04,  2.18it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 392/400 [03:01<00:03,  2.18it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 393/400 [03:01<00:03,  2.18it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 394/400 [03:01<00:02,  2.18it/s]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 395/400 [03:02<00:02,  2.18it/s]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 396/400 [03:02<00:01,  2.18it/s]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 397/400 [03:03<00:01,  2.17it/s]\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 398/400 [03:03<00:00,  2.18it/s]\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 399/400 [03:04<00:00,  2.18it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 400/400 [03:04<00:00,  2.23it/s]\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: str_labels, text. If str_labels, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: str_labels, text. If str_labels, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 3200\n",
      "  Batch size = 64\u001b[0m\n",
      "\u001b[34mNum examples = 3200\n",
      "  Batch size = 64\u001b[0m\n",
      "\u001b[34m0%|          | 0/50 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m4%|▍         | 2/50 [00:00<00:07,  6.41it/s]#033[A\u001b[0m\n",
      "\u001b[34m6%|▌         | 3/50 [00:00<00:10,  4.51it/s]#033[A\u001b[0m\n",
      "\u001b[34m8%|▊         | 4/50 [00:00<00:11,  3.92it/s]#033[A\u001b[0m\n",
      "\u001b[34m10%|█         | 5/50 [00:01<00:12,  3.64it/s]#033[A\u001b[0m\n",
      "\u001b[34m12%|█▏        | 6/50 [00:01<00:12,  3.47it/s]#033[A\u001b[0m\n",
      "\u001b[34m14%|█▍        | 7/50 [00:01<00:12,  3.38it/s]#033[A\u001b[0m\n",
      "\u001b[34m16%|█▌        | 8/50 [00:02<00:12,  3.33it/s]#033[A\u001b[0m\n",
      "\u001b[34m18%|█▊        | 9/50 [00:02<00:12,  3.29it/s]#033[A\u001b[0m\n",
      "\u001b[34m20%|██        | 10/50 [00:02<00:12,  3.25it/s]#033[A\u001b[0m\n",
      "\u001b[34m22%|██▏       | 11/50 [00:03<00:12,  3.23it/s]#033[A\u001b[0m\n",
      "\u001b[34m24%|██▍       | 12/50 [00:03<00:11,  3.20it/s]#033[A\u001b[0m\n",
      "\u001b[34m26%|██▌       | 13/50 [00:03<00:11,  3.19it/s]#033[A\u001b[0m\n",
      "\u001b[34m28%|██▊       | 14/50 [00:04<00:11,  3.19it/s]#033[A\u001b[0m\n",
      "\u001b[34m30%|███       | 15/50 [00:04<00:11,  3.18it/s]#033[A\u001b[0m\n",
      "\u001b[34m32%|███▏      | 16/50 [00:04<00:10,  3.17it/s]#033[A\u001b[0m\n",
      "\u001b[34m34%|███▍      | 17/50 [00:05<00:10,  3.18it/s]#033[A\u001b[0m\n",
      "\u001b[34m36%|███▌      | 18/50 [00:05<00:10,  3.19it/s]#033[A\u001b[0m\n",
      "\u001b[34m38%|███▊      | 19/50 [00:05<00:09,  3.20it/s]#033[A\u001b[0m\n",
      "\u001b[34m40%|████      | 20/50 [00:05<00:09,  3.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m42%|████▏     | 21/50 [00:06<00:09,  3.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m44%|████▍     | 22/50 [00:06<00:08,  3.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m46%|████▌     | 23/50 [00:06<00:08,  3.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m48%|████▊     | 24/50 [00:07<00:08,  3.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m50%|█████     | 25/50 [00:07<00:07,  3.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 26/50 [00:07<00:07,  3.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 27/50 [00:08<00:07,  3.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 28/50 [00:08<00:06,  3.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 29/50 [00:08<00:06,  3.20it/s]#033[A\u001b[0m\n",
      "\u001b[34m60%|██████    | 30/50 [00:09<00:06,  3.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 31/50 [00:09<00:05,  3.20it/s]#033[A\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 32/50 [00:09<00:05,  3.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 33/50 [00:10<00:05,  3.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 34/50 [00:10<00:04,  3.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m70%|███████   | 35/50 [00:10<00:04,  3.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 36/50 [00:10<00:04,  3.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 37/50 [00:11<00:04,  3.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 38/50 [00:11<00:03,  3.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 39/50 [00:11<00:03,  3.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m80%|████████  | 40/50 [00:12<00:03,  3.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 41/50 [00:12<00:02,  3.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 42/50 [00:12<00:02,  3.20it/s]#033[A\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 43/50 [00:13<00:02,  3.19it/s]#033[A\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 44/50 [00:13<00:01,  3.20it/s]#033[A\u001b[0m\n",
      "\u001b[34m90%|█████████ | 45/50 [00:13<00:01,  3.20it/s]#033[A\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 46/50 [00:14<00:01,  3.20it/s]#033[A\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 47/50 [00:14<00:00,  3.20it/s]#033[A\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 48/50 [00:14<00:00,  3.19it/s]#033[A\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 49/50 [00:15<00:00,  3.20it/s]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 50/50 [00:15<00:00,  3.20it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.24509043991565704, 'eval_f1': 0.6472479667108001, 'eval_roc_auc': 0.7555937500000001, 'eval_accuracy': 0.534375, 'eval_runtime': 15.644, 'eval_samples_per_second': 204.551, 'eval_steps_per_second': 3.196, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 400/400 [03:20<00:00,  2.23it/s]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 50/50 [00:15<00:00,  3.20it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m100%|██████████| 400/400 [03:20<00:00,  2.23it/s]\u001b[0m\n",
      "\u001b[34m{'train_runtime': 200.3546, 'train_samples_per_second': 63.872, 'train_steps_per_second': 1.996, 'train_loss': 0.3850246047973633, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 400/400 [03:20<00:00,  2.00it/s]\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: str_labels, text. If str_labels, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: str_labels, text. If str_labels, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 3200\n",
      "  Batch size = 64\u001b[0m\n",
      "\u001b[34mNum examples = 3200\n",
      "  Batch size = 64\u001b[0m\n",
      "\u001b[34m0%|          | 0/50 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 2/50 [00:00<00:07,  6.44it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 3/50 [00:00<00:10,  4.54it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 4/50 [00:00<00:11,  3.93it/s]\u001b[0m\n",
      "\u001b[34m10%|█         | 5/50 [00:01<00:12,  3.65it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 6/50 [00:01<00:12,  3.49it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 7/50 [00:01<00:12,  3.39it/s]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 8/50 [00:02<00:12,  3.32it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 9/50 [00:02<00:12,  3.28it/s]\u001b[0m\n",
      "\u001b[34m20%|██        | 10/50 [00:02<00:12,  3.27it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 11/50 [00:03<00:12,  3.25it/s]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 12/50 [00:03<00:11,  3.24it/s]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 13/50 [00:03<00:11,  3.23it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 14/50 [00:04<00:11,  3.23it/s]\u001b[0m\n",
      "\u001b[34m30%|███       | 15/50 [00:04<00:10,  3.22it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 16/50 [00:04<00:10,  3.22it/s]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 17/50 [00:04<00:10,  3.22it/s]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 18/50 [00:05<00:09,  3.22it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 19/50 [00:05<00:09,  3.21it/s]\u001b[0m\n",
      "\u001b[34m40%|████      | 20/50 [00:05<00:09,  3.22it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 21/50 [00:06<00:09,  3.21it/s]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 22/50 [00:06<00:08,  3.22it/s]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 23/50 [00:06<00:08,  3.21it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 24/50 [00:07<00:08,  3.21it/s]\u001b[0m\n",
      "\u001b[34m50%|█████     | 25/50 [00:07<00:07,  3.21it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 26/50 [00:07<00:07,  3.22it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 27/50 [00:08<00:07,  3.21it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 28/50 [00:08<00:06,  3.21it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 29/50 [00:08<00:06,  3.21it/s]\u001b[0m\n",
      "\u001b[34m60%|██████    | 30/50 [00:09<00:06,  3.21it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 31/50 [00:09<00:05,  3.21it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 32/50 [00:09<00:05,  3.21it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 33/50 [00:09<00:05,  3.21it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 34/50 [00:10<00:04,  3.20it/s]\u001b[0m\n",
      "\u001b[34m70%|███████   | 35/50 [00:10<00:04,  3.21it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 36/50 [00:10<00:04,  3.21it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 37/50 [00:11<00:04,  3.21it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 38/50 [00:11<00:03,  3.21it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 39/50 [00:11<00:03,  3.20it/s]\u001b[0m\n",
      "\u001b[34m80%|████████  | 40/50 [00:12<00:03,  3.20it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 41/50 [00:12<00:02,  3.20it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 42/50 [00:12<00:02,  3.20it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 43/50 [00:13<00:02,  3.20it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 44/50 [00:13<00:01,  3.20it/s]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 45/50 [00:13<00:01,  3.20it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 46/50 [00:14<00:01,  3.20it/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 47/50 [00:14<00:00,  3.20it/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 48/50 [00:14<00:00,  3.21it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 49/50 [00:14<00:00,  3.21it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 50/50 [00:15<00:00,  3.20it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 50/50 [00:15<00:00,  3.27it/s]\u001b[0m\n",
      "\u001b[34m***** Eval results *****\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m2023-10-25 06:51:30,832 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-10-25 06:51:30,833 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-10-25 06:51:30,833 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# access the logs of the training job\n",
    "huggingface_estimator.sagemaker_session.logs_for_job(huggingface_estimator.latest_training_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attach to old training job to an estimator \n",
    "\n",
    "In Sagemaker you can attach an old training job to an estimator to continue training, get results etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "# job which is going to be attached to the estimator\n",
    "old_training_job_name=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attach old training job\n",
    "huggingface_estimator_loaded = Estimator.attach(old_training_job_name)\n",
    "\n",
    "# get model output s3 from training job\n",
    "huggingface_estimator_loaded.model_data"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.g4dn.xlarge",
  "interpreter": {
   "hash": "c281c456f1b8161c8906f4af2c08ed2c40c50136979eaae69688b01f70e9f4a9"
  },
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.13 Python 3.9 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/pytorch-1.13-gpu-py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
